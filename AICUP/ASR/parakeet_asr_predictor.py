import os
import argparse
import nemo.collections.asr as nemo_asr
import librosa
import soundfile as sf
from typing import List, Optional, Dict, Any
from jiwer import mer, wer
from transformers.models.whisper.english_normalizer import BasicTextNormalizer, EnglishTextNormalizer
from tqdm import tqdm
import re
import json
os.environ["CUDA_VISIBLE_DEVICES"] = "0"


def calculate_mer(ground_truth_texts, predicted_texts):
    """ Mix Error Rate (MER) English only"""
    mer_scores = {}
    total_mer = 0
    count = 0

    normalizer = EnglishTextNormalizer({})

    for filename, ref_text in ground_truth_texts.items():
        if filename in predicted_texts:
            pred_text = predicted_texts[filename]
            ref_text = normalizer(ref_text)
            pred_text = normalizer(pred_text)

            mer_score = mer(ref_text, pred_text)
            mer_scores[filename] = mer_score
            total_mer += mer_score
        else:
            mer_scores[filename] = 1
            total_mer += 1
        count += 1

    average_mer = total_mer / count if count != 0 else 0
    return mer_scores, average_mer


class ParakeetASRPredictor:
    """NVIDIA Parakeet TDT 0.6B V2 èªéŸ³è­˜åˆ¥é æ¸¬å™¨ - æ‰¹é‡è™•ç†ç‰ˆ"""

    def __init__(self, model_name: str = "nvidia/parakeet-tdt-0.6b-v2"):
        """
        åˆå§‹åŒ– ASR æ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç¨±ï¼Œé è¨­ç‚º nvidia/parakeet-tdt-0.6b-v2
        """
        model_name = "nvidia/parakeet-tdt-0.6b-v2"
        print(f"ğŸš€ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        print("â³ æ¨¡å‹è¼‰å…¥ä¸­ï¼Œè«‹ç¨å€™...")
        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name)
        # self.asr_model = nemo_asr.models.ASRModel.restore_from(model_name)

        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

    def preprocess_audio(self, audio_path: str, target_sr: int = 16000) -> str:
        """
        é è™•ç†éŸ³é »æ–‡ä»¶ï¼Œç¢ºä¿ç¬¦åˆæ¨¡å‹è¦æ±‚

        Args:
            audio_path: éŸ³é »æ–‡ä»¶è·¯å¾‘
            target_sr: ç›®æ¨™æ¡æ¨£ç‡ (16kHz)

        Returns:
            è™•ç†å¾Œçš„éŸ³é »æ–‡ä»¶è·¯å¾‘
        """
        # è®€å–éŸ³é »
        audio, sr = librosa.load(audio_path, sr=None)

        # æª¢æŸ¥æ˜¯å¦éœ€è¦è™•ç†
        needs_processing = False

        # è½‰æ›ç‚ºå–®è²é“
        if len(audio.shape) > 1:
            audio = librosa.to_mono(audio)
            needs_processing = True

        # é‡æ–°æ¡æ¨£åˆ° 16kHz
        if sr != target_sr:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
            needs_processing = True

        # å¦‚æœä¸éœ€è¦è™•ç†ï¼Œç›´æ¥è¿”å›åŸæ–‡ä»¶
        if not needs_processing:
            return audio_path

        # æ­£ç¢ºè™•ç†æ–‡ä»¶è·¯å¾‘
        base_path, ext = os.path.splitext(audio_path)
        processed_path = f"{base_path}_processed{ext}"

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        processed_dir = os.path.dirname(processed_path)
        if processed_dir:
            os.makedirs(processed_dir, exist_ok=True)

        sf.write(processed_path, audio, target_sr)

        return processed_path

    def load_labels(self, label_file: str) -> Dict[str, str]:
        """
        è®€å–æ¨™ç±¤æ–‡ä»¶

        Args:
            label_file: æ¨™ç±¤æ–‡ä»¶è·¯å¾‘

        Returns:
            æ–‡ä»¶ååˆ°æ–‡æœ¬çš„æ˜ å°„å­—å…¸
        """
        print(f"ğŸ“– æ­£åœ¨è®€å–æ¨™ç±¤æ–‡ä»¶: {label_file}")
        labels = {}
        with open(label_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        for line in tqdm(lines, desc="è®€å–æ¨™ç±¤", unit="è¡Œ"):
            line = line.strip()
            if line:
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    file_id, text = parts
                    # å‡è¨­éŸ³é »æ–‡ä»¶åæ ¼å¼ç‚º {file_id}.wav
                    labels[f"{file_id}.wav"] = text

        print(f"âœ… æˆåŠŸè®€å– {len(labels)} å€‹æ¨™ç±¤")
        return labels

    def batch_transcribe(self, audio_dir: str, output_file: Optional[str] = None,
                         label_file: Optional[str] = None, output_label_file: str = "task1_answer_Finetuned.txt",
                         json_output_file: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è½‰éŒ„ç›®éŒ„ä¸­çš„éŸ³é »æ–‡ä»¶

        Args:
            audio_dir: éŸ³é »æ–‡ä»¶ç›®éŒ„
            output_file: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰
            label_file: æ¨™ç±¤æ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼Œç”¨æ–¼è¨ˆç®—MERï¼‰
            output_label_file: è¼¸å‡ºæ¨™ç±¤æ–‡ä»¶è·¯å¾‘ï¼ˆé è¨­ç‚ºoutput_label.txtï¼‰
            json_output_file: JSONæ™‚é–“æˆ³è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰

        Returns:
            è½‰éŒ„çµæœåˆ—è¡¨
        """
        print(f"ğŸ” æ­£åœ¨æƒæéŸ³é »ç›®éŒ„: {audio_dir}")

        # æ”¯æ´çš„éŸ³é »æ ¼å¼
        supported_formats = ['.wav', '.flac', '.mp3', '.m4a']

        # ç²å–æ‰€æœ‰éŸ³é »æ–‡ä»¶
        audio_files = []
        all_files = os.listdir(audio_dir)

        for file in tqdm(all_files, desc="æƒææ–‡ä»¶", unit="å€‹"):
            if any(file.lower().endswith(fmt) for fmt in supported_formats):
                audio_files.append(os.path.join(audio_dir, file))

        if not audio_files:
            print(f"âŒ åœ¨ç›®éŒ„ {audio_dir} ä¸­æœªæ‰¾åˆ°æ”¯æ´çš„éŸ³é »æ–‡ä»¶")
            print(f"æ”¯æ´æ ¼å¼: {', '.join(supported_formats)}")
            return []

        print(f"âœ… æ‰¾åˆ° {len(audio_files)} å€‹éŸ³é »æ–‡ä»¶")

        # é è™•ç†éŸ³é »æ–‡ä»¶
        print("ğŸ”§ æ­£åœ¨é è™•ç†éŸ³é »æ–‡ä»¶...")
        processed_files = []
        failed_files = []

        for audio_file in tqdm(audio_files, desc="é è™•ç†éŸ³é »", unit="å€‹"):
            try:
                processed_file = self.preprocess_audio(audio_file)
                processed_files.append(processed_file)
            except Exception as e:
                print(f"\nâš ï¸  è™•ç†æ–‡ä»¶ {os.path.basename(audio_file)} æ™‚å‡ºéŒ¯: {e}")
                failed_files.append(audio_file)

        if failed_files:
            print(f"âš ï¸  {len(failed_files)} å€‹æ–‡ä»¶è™•ç†å¤±æ•—")

        if not processed_files:
            print("âŒ æ²’æœ‰æˆåŠŸè™•ç†çš„éŸ³é »æ–‡ä»¶")
            return []

        print(f"âœ… æˆåŠŸé è™•ç† {len(processed_files)} å€‹éŸ³é »æ–‡ä»¶")

        # æ‰¹é‡è½‰éŒ„
        print("ğŸ¤ é–‹å§‹æ‰¹é‡è½‰éŒ„...")
        print("â³ æ­£åœ¨é€²è¡ŒèªéŸ³è­˜åˆ¥ï¼Œè«‹ç¨å€™...")
        outputs = self.asr_model.transcribe(processed_files, timestamps=True, batch_size=4)
        print("âœ… è½‰éŒ„å®Œæˆï¼")

        # æ•´ç†çµæœ
        print("ğŸ“Š æ­£åœ¨æ•´ç†è½‰éŒ„çµæœ...")
        final_results = []

        # éœ€è¦èª¿æ•´ç´¢å¼•ï¼Œå› ç‚ºå¯èƒ½æœ‰å¤±æ•—çš„æ–‡ä»¶
        successful_audio_files = [f for f in audio_files if f not in failed_files]

        # åˆå§‹åŒ–è‹±æ–‡æ–‡æœ¬æ­£è¦åŒ–å™¨
        normalizer = EnglishTextNormalizer({})

        for i, output in enumerate(tqdm(outputs, desc="æ•´ç†çµæœ", unit="å€‹")):
            # ä½¿ç”¨ Whisper çš„è‹±æ–‡æ­£è¦åŒ–å™¨è™•ç†æ–‡æœ¬
            converted_text = normalizer(output.text)

            result = {
                'file': successful_audio_files[i],
                'transcription': converted_text,
                'original_transcription': output.text,  # ä¿ç•™åŸå§‹è½‰éŒ„çµæœ
                'word_timestamps': output.timestamp.get('word', []),
                'segment_timestamps': output.timestamp.get('segment', []),
                'char_timestamps': output.timestamp.get('char', [])
            }
            final_results.append(result)

        # **æ–°å¢ï¼šè¼¸å‡º task1_answer_Finetuned.txt æ–‡ä»¶**
        print(f"ğŸ“„ æ­£åœ¨è¼¸å‡ºæ¨™ç±¤æ–‡ä»¶: {output_label_file}")
        os.makedirs(os.path.dirname(output_label_file), exist_ok=True)
        with open(output_label_file, 'w', encoding='utf-8') as f:
            for result in tqdm(final_results, desc="å¯«å…¥æ¨™ç±¤æ–‡ä»¶", unit="å€‹"):
                filename = os.path.basename(result['file'])
                # ç§»é™¤å‰¯æª”åï¼Œåªä¿ç•™æª”å
                filename_without_ext = os.path.splitext(filename)[0]
                transcription = result['transcription']
                f.write(f"{filename_without_ext}\t{transcription}\n")
        print(f"âœ… æ¨™ç±¤æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {output_label_file}")

        # **ä¿®æ”¹ï¼šè¼¸å‡ºæ–°æ ¼å¼çš„JSONæ™‚é–“æˆ³æ–‡ä»¶**
        if json_output_file:
            print(f"ğŸ“„ æ­£åœ¨è¼¸å‡ºJSONæ™‚é–“æˆ³æ–‡ä»¶: {json_output_file}")

            json_data = {}

            for result in tqdm(final_results, desc="ç”ŸæˆJSONæ™‚é–“æˆ³", unit="å€‹"):
                filename = os.path.basename(result['file'])
                # ç§»é™¤å‰¯æª”åï¼Œåªä¿ç•™æª”å
                filename_without_ext = os.path.splitext(filename)[0]

                # æå–è©ç´šåˆ¥æ™‚é–“æˆ³
                word_timestamps = result['word_timestamps']
                timestamp_list = []

                for word_stamp in word_timestamps:
                    timestamp_entry = {
                        "startTime": word_stamp['start'],
                        "endTime": word_stamp['end'],
                        "Text": word_stamp['word']
                    }
                    timestamp_list.append(timestamp_entry)

                # **æ–°æ ¼å¼ï¼šåŒ…å«transcriptionå’ŒwordTimeStamp**
                json_data[filename_without_ext] = {
                    "transcription": result['transcription'],
                    "wordTimeStamp": timestamp_list
                }

            # å¯«å…¥JSONæª”æ¡ˆ
            with open(json_output_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… JSONæ™‚é–“æˆ³æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {json_output_file}")

        # è¨ˆç®—MERæŒ‡æ¨™ï¼ˆå¦‚æœæä¾›äº†æ¨™ç±¤æ–‡ä»¶ï¼‰
        mer_scores = None
        average_mer = None
        normalized_ground_truth = {}  # æ–°å¢ï¼šå„²å­˜æ­£è¦åŒ–å¾Œçš„æ¨™ç±¤

        if label_file:
            print("ğŸ“ˆ æ­£åœ¨è¨ˆç®—MERæŒ‡æ¨™...")
            ground_truth = self.load_labels(label_file)
            predicted_texts = {}

            for result in tqdm(final_results, desc="æº–å‚™MERè¨ˆç®—", unit="å€‹"):
                filename = os.path.basename(result['file'])
                predicted_texts[filename] = result['transcription']

            print("ğŸ§® è¨ˆç®—MERåˆ†æ•¸ä¸­...")
            mer_scores, average_mer = calculate_mer(ground_truth, predicted_texts)
            print(f"âœ… å¹³å‡MER: {average_mer:.4f}")

            # æ­£è¦åŒ–æ¨™ç±¤æ–‡æœ¬ä¸¦å„²å­˜
            for filename, original_label in ground_truth.items():
                normalized_ground_truth[filename] = normalizer(original_label)

        # ä¿å­˜è©³ç´°çµæœåˆ°æ–‡ä»¶
        if output_file:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜è©³ç´°çµæœåˆ°: {output_file}")
            with open(output_file, 'w', encoding='utf-8') as f:
                # å¯«å…¥MERçµæœï¼ˆå¦‚æœæœ‰ï¼‰
                if mer_scores is not None:
                    f.write(f"å¹³å‡MER: {average_mer:.4f}\n")
                    f.write("=" * 50 + "\n")
                    f.write("å„æ–‡ä»¶MERåˆ†æ•¸:\n")
                    for filename, score in mer_scores.items():
                        f.write(f"{filename}: {score:.4f}\n")
                    f.write("=" * 50 + "\n\n")

                # å¯«å…¥è½‰éŒ„çµæœ
                for result in tqdm(final_results, desc="å¯«å…¥è©³ç´°çµæœ", unit="å€‹"):
                    filename = os.path.basename(result['file'])
                    f.write(f"æ–‡ä»¶: {result['file']}\n")

                    # å¦‚æœæœ‰æ¨™ç±¤æ–‡ä»¶ï¼Œå¯«å…¥æ¨™ç±¤æ–‡æœ¬
                    if label_file and mer_scores is not None:
                        # ground_truth å·²ç¶“åœ¨è¨ˆç®—MERæ™‚è¼‰å…¥éäº†
                        if filename in ground_truth:
                            f.write(f"æ¨™ç±¤æ–‡æœ¬: {ground_truth[filename]}\n")
                            f.write(f"æ­£è¦åŒ–æ¨™ç±¤: {normalized_ground_truth[filename]}\n")
                        else:
                            f.write(f"æ¨™ç±¤æ–‡æœ¬: [æœªæ‰¾åˆ°å°æ‡‰æ¨™ç±¤]\n")

                    f.write(f"åŸå§‹è½‰éŒ„: {result['original_transcription']}\n")
                    f.write(f"æ­£è¦åŒ–å¾Œ: {result['transcription']}\n")

                    # å¦‚æœæœ‰MERåˆ†æ•¸ï¼Œä¹Ÿå¯«å…¥
                    if mer_scores and filename in mer_scores:
                        f.write(f"MER: {mer_scores[filename]:.4f}\n")

                    f.write("æ®µè½æ™‚é–“æˆ³:\n")
                    for stamp in result['segment_timestamps']:
                        f.write(f"  {stamp['start']:.2f}s - {stamp['end']:.2f}s : {stamp['segment']}\n")
                    f.write("è©æ™‚é–“æˆ³:\n")
                    for stamp in result['word_timestamps']:
                        f.write(f"  {stamp['start']:.2f}s - {stamp['end']:.2f}s : {stamp['word']}\n")
                    f.write("\n" + "=" * 50 + "\n\n")
            print(f"âœ… è©³ç´°çµæœå·²æˆåŠŸä¿å­˜åˆ°: {output_file}")

        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        print("ğŸ§¹ æ­£åœ¨æ¸…ç†è‡¨æ™‚æ–‡ä»¶...")
        cleaned_count = 0
        for i, processed_file in enumerate(processed_files):
            if processed_file != successful_audio_files[i]:
                try:
                    os.remove(processed_file)
                    cleaned_count += 1
                except:
                    pass

        if cleaned_count > 0:
            print(f"âœ… å·²æ¸…ç† {cleaned_count} å€‹è‡¨æ™‚æ–‡ä»¶")

        # å°‡MERçµæœåŠ å…¥è¿”å›å€¼
        if mer_scores is not None:
            return final_results, mer_scores, average_mer
        else:
            return final_results


def main():
    parser = argparse.ArgumentParser(description='NVIDIA Parakeet TDT 0.6B V2 æ‰¹é‡èªéŸ³è­˜åˆ¥')
    parser.add_argument('--audio_dir',default="../datasets/Test/audio_NoBGM_16k", type=str, help='éŸ³é »æ–‡ä»¶ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--output', type=str, help='è©³ç´°è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--labels',default="../datasets/val/task1_answer.txt", type=str, help='æ¨™ç±¤æ–‡ä»¶è·¯å¾‘ï¼ˆç”¨æ–¼è¨ˆç®—MERï¼‰')
    parser.add_argument('--output_label', type=str, default='Test_result/task1_answer_noFinetuned.txt',
                        help='è¼¸å‡ºæ¨™ç±¤æ–‡ä»¶è·¯å¾‘ï¼ˆé è¨­ç‚ºoutput_label.txtï¼‰')
    parser.add_argument('--json_output',default="Test_result/task1_answer_noFinetuned_timeStamp.json", type=str, help='JSONæ™‚é–“æˆ³è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--model', type=str, default=r'C:\Users\C110151154\PycharmProjects\AICUP\AICUP\ASR\fintuned_model\Speech_To_Text_Finetuning.nemo', help='æ¨¡å‹åç¨±')

    args = parser.parse_args()
    # åˆå§‹åŒ–é æ¸¬å™¨
    predictor = ParakeetASRPredictor(model_name=args.model)

    # æ‰¹é‡è½‰éŒ„
    print(f"ğŸ¯ é–‹å§‹æ‰¹é‡è½‰éŒ„ä»»å‹™")
    print(f"ğŸ“ ç›®æ¨™ç›®éŒ„: {args.audio_dir}")
    if args.output:
        print(f"ğŸ“„ è©³ç´°è¼¸å‡ºæ–‡ä»¶: {args.output}")
    print(f"ğŸ·ï¸  æ¨™ç±¤è¼¸å‡ºæ–‡ä»¶: {args.output_label}")
    if args.labels:
        print(f"ğŸ·ï¸  åƒè€ƒæ¨™ç±¤æ–‡ä»¶: {args.labels}")
    if args.json_output:
        print(f"ğŸ“Š JSONæ™‚é–“æˆ³æ–‡ä»¶: {args.json_output}")
    print("-" * 50)

    result = predictor.batch_transcribe(
        args.audio_dir,
        args.output,
        args.labels,
        args.output_label,
        args.json_output
    )

    # è™•ç†è¿”å›çµæœ
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰¹é‡è½‰éŒ„ä»»å‹™å®Œæˆï¼")
    print("=" * 60)
    args.labels = None
    if args.labels:
        results, mer_scores, average_mer = result
        # è¼‰å…¥æ¨™ç±¤æ–‡ä»¶ç”¨æ–¼é¡¯ç¤º
        ground_truth = predictor.load_labels(args.labels)

        print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸè™•ç†: {len(results)} å€‹æ–‡ä»¶")
        print(f"   ğŸ“ˆ å¹³å‡MER: {average_mer:.4f}")
        print(f"   ğŸ“„ æ¨™ç±¤æ–‡ä»¶: {args.output_label}")
        if args.json_output:
            print(f"   ğŸ“Š JSONæ™‚é–“æˆ³: {args.json_output}")

        print(f"\nğŸ“ è½‰éŒ„çµæœé è¦½ (å‰5å€‹æ–‡ä»¶):")
        print("-" * 50)
        for i, result_item in enumerate(results[:5]):
            filename = os.path.basename(result_item['file'])
            print(f"ğŸµ {i + 1}. {filename}")

            # å¦‚æœæœ‰æ¨™ç±¤ï¼Œé¡¯ç¤ºæ¨™ç±¤æ–‡æœ¬
            if filename in ground_truth:
                print(f"   ğŸ·ï¸  æ¨™ç±¤: {ground_truth[filename][:80]}...")

            print(f"   ğŸ“ åŸå§‹: {result_item['original_transcription'][:80]}...")
            print(f"   ğŸ”¢ æ­£è¦åŒ–: {result_item['transcription'][:80]}...")
            if filename in mer_scores:
                print(f"   ğŸ“Š MER: {mer_scores[filename]:.4f}")
            print()
    else:
        results = result
        print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸè™•ç†: {len(results)} å€‹æ–‡ä»¶")
        print(f"   ğŸ“„ æ¨™ç±¤æ–‡ä»¶: {args.output_label}")
        if args.json_output:
            print(f"   ğŸ“Š JSONæ™‚é–“æˆ³: {args.json_output}")

        print(f"\nğŸ“ è½‰éŒ„çµæœé è¦½ (å‰5å€‹æ–‡ä»¶):")
        print("-" * 50)
        for i, result_item in enumerate(results[:5]):
            filename = os.path.basename(result_item['file'])
            print(f"ğŸµ {i + 1}. {filename}")
            print(f"   ğŸ“ åŸå§‹: {result_item['original_transcription'][:80]}...")
            print(f"   ğŸ”¢ æ­£è¦åŒ–: {result_item['transcription'][:80]}...")
            print()

    if len(results) > 5:
        print(f"... é‚„æœ‰ {len(results) - 5} å€‹æ–‡ä»¶çš„çµæœ")

    print("ğŸŠ ä»»å‹™å®Œæˆï¼")


if __name__ == "__main__":
    main()
